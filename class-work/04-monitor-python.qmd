---
title: Monitor your model
subtitle: Deploy and maintain models with vetiver (Python)
format: html
---

## Data for modeling & data for monitoring ü§ù

```{python}
import pandas as pd

inspections = pd.read_parquet('../data/inspections.parquet')
inspections_new = pd.read_parquet('../data/inspections_monitoring.parquet')
```

Compare the modeling and monitoring datasets:

```{python}
## make a plot or table
```

## Monitor metrics over time ‚è≥

```{python}
#| fig-align: center
from vetiver import vetiver_endpoint, predict, compute_metrics, plot_metrics
from sklearn.metrics import recall_score, accuracy_score
from datetime import timedelta

inspections_new['inspection_date'] = pd.to_datetime(inspections_new['inspection_date'])
inspections_new['month'] = inspections_new['inspection_date'].dt.month
inspections_new['year'] = inspections_new['inspection_date'].dt.year

url = "https://colorado.posit.co/rsc/chicago-inspections-python/predict"
endpoint = vetiver_endpoint(url)
inspections_new["preds"] = predict(endpoint = url, data = inspections_new.drop(columns = ["results", "aka_name", "inspection_date"]))

inspections_new["preds"] = inspections_new["preds"].map({"PASS": 0, "FAIL": 1})
inspections_new["results"] = inspections_new["results"].map({"PASS": 0, "FAIL": 1})

td = timedelta(weeks = 4)
metric_set = [accuracy_score, recall_score]

metrics_df = compute_metrics(
    data = ___,
    date_var = "inspection_date", 
    period = td,
    metric_set = metric_set,
    truth = "results",
    estimate = "preds"
  )
metrics_df
```

```{python}
plot_metrics(metrics_df)
```

Read about how you can use pins for versioning metrics: <https://rstudio.github.io/vetiver-python/stable/reference/pin_metrics.html>

## ML metrics ‚û°Ô∏è organizational outcomes

What proportion of failed inspections were predicted to fail?

```{python}
by_facility_type = pd.DataFrame()
for metric in metric_set:
    by_facility_type[metric.__qualname__] = inspections_new.groupby(___)\
        .apply(lambda x: metric(y_pred=x["preds"], y_true=x["results"]))
by_facility_type
```

```{python}
## make a visualization
```

## Create a monitoring dashboard or report üìä

To wrap up, create a model monitoring dashboard or report and publish it to Connect. You have a few options:

- Open the `04-monitor-report.qmd`, make any changes you like (for example, different metrics or visualizations), render it, and publish to Connect.
- Open a fresh Jupyter Notebook or Quarto file. Update to make a report for _your_ model and publish to Connect.
