---
title: "1 - Introduction"
subtitle: "Deploy and maintain models with vetiver"
format:
  revealjs: 
    slide-number: true
    footer: <https://posit-conf-2023.github.io/vetiver>
    preview-links: auto
    incremental: true
    theme: [default]
    width: 1280
    height: 720
knitr:
  opts_chunk: 
    echo: true
    collapse: true
    comment: "#>"
---

```{r}
#| include: false
#| file: setup.R
```

::: r-fit-text
Welcome!
:::

::: columns
::: {.column width="50%"}

<center>

### {{< fa wifi >}} 

Wi-Fi network name

`tktk`

</center>

:::

::: {.column width="50%"}

<center>

### {{< fa key >}} 

Wi-Fi password

`tktk`

</center>

:::
:::


## Who are you?

-   You have intermediate R or Python knowledge

-   You can read data from CSV and other flat files, transform and reshape data, and make a wide variety of graphs

-   You can fit a model to data with your modeling framework of choice
wide variety of graphs

-   You have exposure to basic modeling and machine learning practice

-   You do **not** need expert familiarity with advanced ML or MLOps topics


## Who are we?

. . .

::: columns
::: {.column width="50%"}
<center>

<img src="https://github.com/isabelizimm.png" style="border-radius: 50%;" width="300px"/>

[{{< fa brands github >}} \@isabelizimm](https://github.com/isabelizimm)

[{{< fa brands mastodon >}} \@isabelizimm@fosstodon.org](https://fosstodon.org/@isabelizimm)

[{{< fa link >}} isabelizimm.github.io](https://isabelizimm.github.io/)

</center>
:::

::: {.column width="50%"}
<center>

<img src="https://github.com/juliasilge.png" style="border-radius: 50%;" width="300px"/>

[{{< fa brands github >}} \@juliasilge](https://github.com/juliasilge)

[{{< fa brands mastodon >}} \@juliasilge@fosstodon.org](https://fosstodon.org/@juliasilge)

[{{< fa brands youtube >}} youtube.com/juliasilge](https://www.youtube.com/juliasilge)

[{{< fa link >}} juliasilge.com](https://juliasilge.com/)

</center>
:::
:::

# Asking for help

. . .

üü™ "I'm stuck and need help!"

üü© "I finished the exercise"


## Plan for this workshop

- *Versioning*
    * Managing change in models ‚úÖ 
- *Deploying*
    * Putting models in REST APIs üéØ
- *Monitoring*
    * Tracking model performance üëÄ

# Introduce yourself to your neighbors üëã

## What is machine learning?

![](images/ml_illustration.jpg){fig-align="center"}

::: footer
Illustration credit: <https://vas3k.com/blog/machine_learning/>
:::

## What is machine learning?

![](images/modeling-process.svg){fig-align="center"}

::: footer
Illustration credit: Chapter 1 of [*Tidy Modeling with R*](https://www.tmwr.org/software-modeling.html)
:::

## MLOps is...

. . .

![](images/ai-landscape.jpg){fig-align="center"}

::: footer
:::

## MLOps is... 

. . .

a set of <u>practices</u> to *deploy* and *maintain* machine learning models in production **reliably** and **efficiently**


## {background-color="white" background-image="https://vetiver.rstudio.com/images/ml_ops_cycle.png" background-size="contain"}

::: footer
:::

## MLOps with vetiver 

> Vetiver, the oil of tranquility, is used as a stabilizing ingredient in perfumery to preserve more volatile fragrances.


# If you develop a model...

. . .

you can operationalize that model!

# If you develop a model...

. . .

you likely *should* operationalize that model!


## Your turn üè∫ {transition="slide-in"}

. . .

*What language does your team use for machine learning?*

*What kinds of models do you commonly use?*

*Have you ever deployed a model?*

```{r}
#| echo: false
countdown(minutes = 3)
```


##  {background-image="https://upload.wikimedia.org/wikipedia/commons/0/0b/AbaloneInside.jpg" background-size="70%"}

## Abalone ages

::: {.nonincremental}
-   Age of abalone can be determined by cutting the shell and counting the number of rings through a microscope
-   Can other measurements be used to determine age?
-   Data from _The Population Biology of Abalone (Haliotis species) in Tasmania. I. Blacklip Abalone (H. rubra) from the North Coast and the Islands of Bass Strait_ by Nash et al (1994)
:::

:::: {.columns}

::: {.column width="50%"}

```{r}
library(tidyverse)
abalone <- read_csv("abalone.csv")
```

:::

::: {.column width="50%"}

```{python}
import pandas as pd
abalone = pd.read_csv("abalone.csv")  
```

:::

::::

## Abalone ages

-   `N = 4177`
-   A numeric outcome, `rings`
-   Other variables to use for prediction:
-   `sex` is a **nominal** predictor
-   `shucked_weight` and `diameter` are **numeric** predictors


## Abalone ages

```{r}
#| echo: false
gt::gt(slice_sample(abalone, n = 15))
```


## Your turn üè∫ {transition="slide-in"}

. . .

*Explore the `abalone` data on your own!*

* *What's the distribution of the outcome, rings?*
* *What's the distribution of numeric variables like weight?*
* *How do rings differ across sex?*

```{r}
#| echo: false
countdown(minutes = 8, id = "explore-abalone")
```

::: notes
Make a plot or summary and then share with neighbor
:::

## 

```{r}
#| fig-align: 'center'
ggplot(abalone, aes(rings)) +
  geom_histogram(bins = 15)
```

:::notes
This histogram brings up a concern. What if we split our data into training/testing and we get unlucky and sample few or none of these large values? That could mean that our model wouldn't be able to predict such values. Let's come back to that!
:::

## 

```{r}
#| fig-align: 'center'
## TODO: replace with a Python plot
ggplot(abalone, aes(rings, sex, fill = sex)) +
  geom_boxplot(alpha = 0.5, show.legend = FALSE)
```

## 

```{r}
#| fig-align: 'center'
abalone |>
  ggplot(aes(shucked_weight, rings, color = shell_weight)) +
  geom_point(alpha = 0.5) +
  scale_color_viridis_c()
```

## Time for machine learning!

```{r tree-example}
#| echo: false
#| fig.width: 16
#| fig.height: 8
#| fig-align: 'center'
#| dev-args: list(bg = "transparent")
tree_mod <- 
  rpart::rpart(
    rings ~ .,
    data = abalone,
    control = rpart::rpart.control(maxdepth = 3, cp = 0.001)
  ) |> 
  partykit::as.party()
plot(tree_mod)
```


## Spend your data budget

### R

```{r}
library(tidymodels)
set.seed(123)
ring_split <- initial_split(abalone, prop = 0.8, strata = rings)
ring_train <- training(ring_split)
ring_test <- testing(ring_split)
```

### Python

```{python}
## TODO set a seed
from sklearn import model_selection
X, y = abalone.iloc[:,:8], abalone['rings']
X_train, X_test, y_train, y_test = model_selection.train_test_split(
    X, y,
    test_size = 0.2
)
```


## Fit a decision tree üå≥

Or your model of choice!

### R

```{r}
abalone_fit <-
  workflow(
    rings ~ ., 
    decision_tree(mode = "regression")
  ) |> 
  fit(data = ring_train) 
```

### Python

```{python}
from sklearn import preprocessing, tree, pipeline
le = preprocessing.OrdinalEncoder().fit(X_train)
tr = tree.DecisionTreeRegressor().fit(le.transform(X_train), y_train)
abalone_fit = pipeline.Pipeline([('label_encoder', le), ('decision_tree', tr)])
```

## Bind preprocessors and models together

![](images/good_workflow.png){fig-align="center"}


## What is wrong with this? {.annotation}

![](images/bad_workflow.png){fig-align="center"}


## Your turn üè∫ {transition="slide-in"}

. . .

* *Split your data in training and testing.*
* *Fit a model to your training data.*

```{r}
#| echo: false
countdown(minutes = 5, id = "first-model")
```



