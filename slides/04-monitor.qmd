---
title: "4 - Monitor your model"
subtitle: "Deploy and maintain models with vetiver"
format:
  revealjs: 
    slide-number: true
    footer: <https://posit-conf-2023.github.io/vetiver>
    preview-links: auto
    incremental: true
    theme: [default, styles.scss]
    width: 1280
    height: 720
knitr:
  opts_chunk: 
    echo: true
    collapse: true
    comment: "#>"
---

```{r}
#| include: false
#| file: setup.R
```

## Plan for this workshop

::: nonincremental
-   *Versioning*
    -   Managing change in models ‚úÖ
-   *Deploying*
    -   Putting models in REST APIs üéØ
-   *Monitoring*
    -   Tracking model performance üëÄ
:::

## Data for model development

Data that you use while building a model for training/testing

### R

```{r}
library(arrow)
path <- here::here("data", "inspections.parquet")
inspections <- read_parquet(path)
```

### Python

```{python}
import pandas as pd
inspections = pd.read_parquet('../data/inspections.parquet')
```


## Data for model monitoring

New data that you predict on after your model deployed

### R

```{r}
library(arrow)
path <- here::here("data", "inspections_monitoring.parquet")
inspections_new <- read_parquet(path)
```

### Python

```{python}
import pandas as pd
inspections_new = pd.read_parquet('../data/inspections_monitoring.parquet')
inspections_new['inspection_date'] = pd.to_datetime(inspections_new['inspection_date'])
inspections_new['month'] = inspections_new['inspection_date'].dt.month
inspections_new['year'] = inspections_new['inspection_date'].dt.year
```

## Data for model monitoring

```{r}
#| echo: false
#| fig-width: 9
#| fig-height: 4.5
#| fig-align: center
library(tidyverse)

bind_rows(inspections |> mutate(monitoring = "Data available at training time"),
          inspections_new |> mutate(monitoring = "New data")) |> 
  ggplot(aes(inspection_date)) +
  geom_histogram(alpha = 0.8, fill = "midnightblue", bins = 100) +
  facet_wrap(vars(monitoring), nrow = 2) +
  theme_light() +
  labs(x = NULL)
```


## My model is performing well!

. . .

üë©üèº‚Äçüîß My model returns predictions quickly, doesn't use too much memory or processing power, and doesn't have outages.

. . .

::: {.callout-tip icon=false}

## Metrics

- latency
- memory and CPU usage
- uptime


:::

## My model is performing well! 

. . .

üë©üèΩ‚Äçüî¨ My model returns predictions that are close to the true values for the predicted quantity.

. . .

::: {.callout-tip icon=false}

## Metrics

- accuracy
- ROC AUC
- F1 score
- RMSE
- log loss

:::

## Model drift üìâ {auto-animate=true}

. . .

::: {.r-fit-text}
DATA drift
:::

## Model drift üìâ {auto-animate=true}

. . .

::: {.r-fit-text}
CONCEPT drift
:::

# When should you retrain your model? üßê

## Your turn üè∫ {transition="slide-in"}

::: {.callout-note icon=false}

## Activity

Using our Chicago data, what could be an example of data drift? Concept drift?

:::

```{r}
#| echo: false
library(countdown)
countdown(minutes = 5)
```

## Learn more about model monitoring on tktk

Come hear Julia's talk on reliable maintenance of ML models on tktk in tktk!

# Monitor your model's **inputs**

## Monitor your model's inputs

Typically it is most useful to compare to your _model development_ data^[Remember you can store metrics of any kind from model development as metadata!]

- Statistical distribution of features individually
- Statistical characteristics of features as a whole
    - Applicability scores: <https://applicable.tidymodels.org/>

## Monitor your model's inputs

```{r}
#| echo: false
#| fig-align: center
bind_rows(
  inspections |> mutate(monitor = "Training/testing"),
  inspections_new |> mutate(monitor = "Monitoring")
) |> 
  ggplot(aes(total_violations, after_stat(density), fill = monitor)) +
  geom_histogram(alpha = 0.5, position = "identity") +
  labs(fill = NULL)
```

## Monitor your model's inputs

```{r}
#| echo: false
#| fig-align: center
bind_rows(
  inspections |> mutate(monitor = "Training/testing"),
  inspections_new |> mutate(monitor = "Monitoring")
) |> 
  count(monitor, facility_type) |> 
  group_by(monitor) |> 
  mutate(proportion = n / sum(n)) |> 
  ggplot(aes(facility_type, proportion, fill = monitor)) +
  geom_col(alpha = 0.8, position = "dodge") +
  scale_y_continuous(labels = scales::percent) +
  labs(fill = NULL, y = "% of establishments", x = NULL)
```

## Your turn üè∫ {transition="slide-in"}

::: {.callout-note icon=false}

## Activity

Create a plot or table comparing the development vs. monitoring distributions of a model input/feature.

How might you make this comparison if you didn't have all the model development data available when monitoring?

What summary statistics might you record during model development, to prepare for monitoring?

:::

```{r}
#| echo: false
library(countdown)
countdown(minutes = 7)
```


# Monitor your model's **outputs**

## Monitor your model's outputs

- In some ML systems, you eventually get a "real" result after making a prediction
- If the Chicago of Public Health used a model like this one, they would get:
    - A prediction from their model
    - A real inspection result after they did their inspection
- In this case, we can monitor our model's statistical performance    
- If you don't ever get a "real" result, you can still monitor the distribution of your outputs


## Monitor your model's outputs

### Python

```{python}
#| fig-align: center
#| output-location: slide 
from vetiver import vetiver_endpoint, predict, compute_metrics, plot_metrics
from sklearn.metrics import recall_score, accuracy_score
from datetime import timedelta

url = "https://colorado.posit.co/rsc/chicago-inspections-python/predict"
endpoint = vetiver_endpoint(url)
inspections_new["preds"] = predict(endpoint = url, data = inspections_new.drop(columns = ["results", "aka_name", "inspection_date"]))

inspections_new["preds"] = inspections_new["preds"].map({"PASS": 1, "FAIL": 0})
inspections_new["results"] = inspections_new["results"].map({"PASS": 1, "FAIL": 0})

td = timedelta(weeks = 4)
metric_set = [accuracy_score, recall_score]

m = compute_metrics(
    data = inspections_new,
    date_var = "inspection_date", 
    period = td,
    metric_set = metric_set,
    truth = "results",
    estimate = "preds"
  )
  
metrics_plot = plot_metrics(m)
```


```{python}
#| echo: false
import plotly.io as pio
pio.write_html(metrics_plot, file="plot.html", auto_open=False)
```

```{r}
#| echo: false
#| eval: true
htmltools::includeHTML("plot.html")
```

## Monitor your model's outputs

### R

```{r}
#| fig-align: center
#| output-location: slide 
library(vetiver)
library(tidymodels)
library(workflows)
url <- "https://colorado.posit.co/rsc/chicago-inspections-rstats/predict"
endpoint <- vetiver_endpoint(url)

augment(endpoint, new_data = inspections_new) |>
  mutate(.pred_class = as.factor(.pred_class)) |> 
  vetiver_compute_metrics(
    inspection_date, 
    "month", 
    results, 
    .pred_class,
    metric_set = metric_set(accuracy, sensitivity)
  ) |> 
  vetiver_plot_metrics()
```

## Your turn üè∫ {transition="slide-in"}

::: {.callout-note icon=false}

## Activity

Use the functions for metrics monitoring from vetiver to create a monitoring visualization.

Choose a different set of metrics or time aggregation.

Note that there are functions for using pins as a way to version and update monitoring results too!

:::

```{r}
#| echo: false
library(countdown)
countdown(minutes = 5)
```


# Feedback loops üîÅ

Deployment of an ML model may *alter* the training data 

- Movie recommendation systems on Netflix, Disney+, Hulu, etc
- Identifying fraudulent credit card transactions at Stripe
- Recidivism models

::: footer
[*Building Machine Learning Pipelines* by Hapke & Nelson](https://www.oreilly.com/library/view/building-machine-learning/9781492053187/)
:::

# Feedback loops can have unexpected consequences

::: footer
[*Building Machine Learning Pipelines* by Hapke & Nelson](https://www.oreilly.com/library/view/building-machine-learning/9781492053187/)
:::

## Feedback loops üîÅ

- Users take some action as a result of a prediction
- Users rate or correct the quality of a prediction
- Produce annotations (crowdsource or expert)
- Produce feedback automatically

::: footer
[*Building Machine Learning Pipelines* by Hapke & Nelson](https://www.oreilly.com/library/view/building-machine-learning/9781492053187/)
:::

## Your turn üè∫ {transition="slide-in"}

::: {.callout-note icon=false}

## Activity

What is a possible feedback loop for the Chicago inspections data?

Do you think your example would be harmful or helpful?

:::

```{r}
#| echo: false
library(countdown)
countdown(minutes = 5)
```


## ML metrics ‚û°Ô∏è organizational outcomes

- Are machine learning metrics like F1 score or accuracy what matter to your organization?
- Consider how ML metrics are related to important outcomes or KPIs for your business or org
- There isn't always a 1-to-1 mapping üòî
- You can **partner** with stakeholders to monitor what's truly important about your model

## Your turn üè∫ {transition="slide-in"}

::: {.callout-note icon=false}

## Activity

Let's say that the most important organizational outcome for the Chicago Department of Public Health is what proportion of failed inspections were predicted to fail.

If "fail" is the event we are predicting, this is the **sensitivity**.

Compute this quantity with the monitoring data, and aggregate by week/month or facility type.

For extra credit, make a visualization showing your results.

:::

```{r}
#| echo: false
countdown(minutes = 7)
```


## ML metrics ‚û°Ô∏è organizational outcomes

::: panel-tabset

### R

```{r}
augment(endpoint, inspections_new) |> 
  mutate(.pred_class = as.factor(.pred_class)) |> 
  group_by(facility_type) |> 
  sensitivity(results, .pred_class)
```

### Python

```{python}
by_facility_type = pd.DataFrame()
for metric in metric_set:
    by_facility_type[metric.__qualname__] = inspections_new.groupby("facility_type")\
        .apply(lambda x: metric(y_pred=x["preds"], y_true=x["results"]))
by_facility_type
```

:::

## What is going on with these inspections?! üò≥

. . .

```{r}
#| fig-align: 'center'
#| echo: false
bind_rows(
  inspections |> mutate(monitor = "Training/testing"),
  inspections_new |> mutate(monitor = "Monitoring")
) |> 
  group_by(monitor, 
           inspection_date = floor_date(inspection_date, unit = "month")) |> 
  summarise(results = mean(results == "PASS")) |> 
  ggplot(aes(inspection_date, results, color = monitor)) +
  geom_line(alpha = 0.8, linewidth = 1.5) +
  scale_y_continuous(labels = scales::percent) +
  labs(y = "% of inspections that have a PASS result", 
       x = NULL, 
       color = NULL)
```


## Possible model monitoring artifacts

- Adhoc analysis that you post in Slack
- Report that you share in Google Drive
- Fully automated dashboard published to Posit Connect

::: footer
[5 Levels of MLOps Maturity](https://www.nannyml.com/blog/5-levels-of-mlops-maturity)
:::

## Possible model monitoring artifacts

![](images/vetiver-templates.png){fig-align="center"}

## Your turn üè∫ {transition="slide-in"}

::: {.callout-note icon=false}

## Activity

Create a Quarto report or R Markdown dashboard for model monitoring.

Keep in mind that we are seeing some real ‚ú®problems‚ú® with this model we trained.

Publish your document to Connect.

:::

```{r}
#| echo: false
library(countdown)
countdown(minutes = 15)
```

## 

::: r-fit-text
We made it! üéâ
:::

## Your turn üè∫ {transition="slide-in"}

::: {.callout-note icon=false}

## Activity

What is one thing you learned that surprised you?

What is one thing you learned that you plan to use?

:::

```{r}
#| echo: false
library(countdown)
countdown(minutes = 5)
```

## Resources to keep learning

-   Documentation at <https://vetiver.rstudio.com/>

-   Isabel's talk from rstudio::conf() 2022 on [Demystifying MLOps](https://www.rstudio.com/conference/2022/talks/demystifying-mlops/)


-   End-to-end demos from Posit Solution Engineering in [R](https://github.com/sol-eng/bike_predict) and [Python](https://github.com/sol-eng/bike_predict_python)

-   Are you on the right track with your MLOps system? Use the rubric in ["The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction"](https://research.google/pubs/pub46555/) by Breck et al (2017)

- Want to learn about how MLOps is being practiced? Read one of our favorite üòç papers of the last year, ["Operationalizing Machine Learning: An Interview Study"](https://arxiv.org/abs/2209.09125) by Shankar et al (20222)

. . .

Follow Posit and/or us on your preferred social media for updates!

# Submit feedback before you leave üó≥Ô∏è

[feedback-url-goes-here](tktk)

Your feedback is crucial! Data from the survey informs curriculum and format decisions for future conf workshops and we really appreciate you taking the time to provide it.
